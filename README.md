## В этом проекте я реализовал задачу сбора данных с помощью парсинга, обработки данных, статистический анализ и последующее обучение модели машинного обучения регрессии на полученных данных для предсказания оценок рейтинга.

### Парсинг данных:

Сбор данных осуществлялся с сайта 'https://kinogo.biz', через браузер 'Chrome' и программу 'ChromeDriver'.

Для этого использовалсь библиотеки и модули Python:

1. selenium, webdriver, Service

Selenium - это библиотека для автоматизации веб-браузера. Она позволяет программистам писать скрипты на языке Python, которые могут управлять браузером, открывать веб-страницы, заполнять формы, кликать на элементы страницы и многое другое.

WebDriver - это интерфейс, который предоставляет Selenium для управления веб-браузером. Он является стандартным способом для автоматизации браузера в Selenium, и позволяет программистам писать скрипты на Python, которые могут управлять браузером.

Service - это класс в библиотеке Selenium, который предоставляет способ управлять фоновыми процессами, такими как драйверы браузера. Это может быть полезно при работе с драйверами в контексте сервера или в других сценариях, где необходимо контролировать процессы драйвера браузера.

2. time

Модуль time предоставляет функции для работы со временем. Он позволяет измерять прошедшее время, задерживать выполнение программы на определенное количество секунд, работать со временными метками и датами.

3. bs4, BeautifulSoup

Библиотека BeautifulSoup(bs4) предназначенная для извлечения данных из HTML- и XML-документов. Она позволяет написать код, который анализирует HTML-страницы и извлекает нужную информацию, такую как заголовки, ссылки, текст и другие элементы страницы.

4. pandas

Библиотека pandas предназначенная для работы с данными и анализа данных.

5. fake_useragent, UserAgent

Для того, чтобы не срабатывала защита сайта, использовалась рандомная маска параметров компьютера с помощью модуля 'UserAgent' библиотеки 'fake_useragent' и отключение функции браузера AutomationControlled, которая определяет используется браузер в автоматическом режиме или человеком.

Далее код извлекает информацию о фильмах с веб-сайта https://kinogo.biz и сохраняет ее в список data.
Он перебирает все страницы сайта с фильмами в цикле, начиная с первой страницы и заканчивая страницей с номером 3381.

Для каждой страницы сайта он загружает HTML-код страницы в объект soup с помощью библиотеки BeautifulSoup.
Затем он находит все блоки HTML-кода с помощью метода find_all и сохраняет их в переменную films.

Затем для каждого блока film он извлекает различную информацию о фильме, такую как название, год выпуска, страна, жанр, продолжительность, качество, количество просмотров, рейтинги на различных сайтах и т. д.

Информация извлекается с помощью методов find и find_all, которые находят элементы HTML-кода по тегу и классу, а затем методы text и split извлекают текст из найденных элементов и разбивают его на отдельные части.

Наконец, полученные данные сохраняются в список data в виде отдельных записей, каждая из которых содержит информацию об одном фильме. Каждая запись представляет собой список, который содержит информацию о фильме.

Внутри цикла for также есть дополнительный внутренний цикл, который обрабатывает каждый фильм в блоке film и устанавливает значения переменных в None, если информация не найдена. Каждый извлеченный элемент записывается в словарь info_dict в виде ключ-значение, где ключ - это категория информации, а значение - это соответствующее значение.

Далее создается датафрейм из собранных данных и сохраняется в файле 'kino.csv'

### Предсказание оценок рейтинга с помощью модели машинного обучения регрессии

### В начале провел EDA

1. Обработал данные для перевода из категориального типа в числовой.
2. Перевел необходимые признаки в числовой вид для дальнейшего обучения регрессии.
3. Целевую переменную вывел путем объединения трех признаков рейтинга с разных сайтов по методу среднего значения.
4. Заполнил пропущенные значения средним, там где было допустимо просто удалил пропуски.
5. Использовал метод 'One-hot encoder' для биноризации категориальных признаков.
6. Создал новые признаки, которые объдинили в себе похожие признапи для уменьшения размерности датасета.
7. Так же некоторы новын признаки объединили в себе не популярные признаки встречающиеся у < 1% объектов.
8. Затем удалил сташие после появления новых признаков ненужные старые.
9. Оценил наличие корреляции между целевым и остальными признаками.

### Подготовка данных для машинного обучения

1. Создал переменную SEED обозначающую параметр random state для более удобного заполнения параметров машинного обучения.
2. Передал переменнй Х все не целевые признаки имеющие числовой формат.
3. Передал переменной у целевую переменную.
4. Разделил данные на тренировочные 80% и тестовые 20% с помощью train_test_split
5. Стандартизировал тренировочные данные с помощью: StandardScaler - это класс, который используется для масштабирования данных. Он преобразует каждый признак (столбец) данных путем вычитания среднего значения этого признака и деления на стандартное отклонение. Это позволяет привести данные к стандартному нормальному распределению со средним значением равным 0 и стандартным отклонением равным 1.
6. Отмасштабировал и тренировочные и тестовые данные с помощью метода transform, который применяет стандартизацию к данным, используя параметры, вычисленные на первом этапе. Это делается для того, чтобы избежать утечки информации из тестовых данных в обучающие данные, что может привести к переобучению модели.

### Машинное обучение

1. LinearRegression - это метод машинного обучения для моделирования отношений между зависимыми и независимыми переменными. В задачах регрессии мы стремимся предсказать непрерывную целевую переменную на основе одного или нескольких признаков.

Модель линейной регрессии строится на основе линейной комбинации признаков и коэффициентов регрессии, которые соответствуют весу каждого признака. Эти коэффициенты регрессии настраиваются на основе обучающих данных, чтобы минимизировать среднеквадратичную ошибку между фактическими и предсказанными значениями целевой переменной.

2. Decision Tree - это модель машинного обучения, которая строит дерево решений, представляющее собой последовательность решений (вопросов), приводящих к конкретному выводу. Дерево решений состоит из узлов и ребер, где узлы представляют собой тесты на признаки, а ребра - ответы на эти тесты.

Алгоритм дерева решений строит дерево поэтапно. На каждом шаге он выбирает тест на один из признаков, который максимально уменьшает неопределенность в данных. Неопределенность может быть измерена с помощью различных метрик, таких как энтропия или неоднородность Джини. После выбора теста данные разделяются на две или более группы в зависимости от результата теста. Этот процесс повторяется рекурсивно, пока все данные не будут разделены на группы одного класса или будет достигнут критерий остановки.

Деревья решений являются мощным инструментом для анализа данных и принятия решений, так как они могут обрабатывать данные с различными типами переменных, включая категориальные и числовые, а также умеют автоматически отбирать наиболее важные признаки. Однако, деревья решений имеют тенденцию к переобучению, что может привести к плохой обобщающей способности на новых данных, поэтому важно правильно настраивать гиперпараметры модели и использовать методы ансамблирования, такие как случайный лес или градиентный бустинг, для улучшения качества предсказаний.

3. Gradient Boosting - это метод машинного обучения, который использует композицию (объединение) нескольких слабых моделей (например, решающих деревьев) для создания одной сильной модели.

Основная идея заключается в том, чтобы последовательно добавлять новые модели в композицию, каждая из которых исправляет ошибки предыдущей модели. При этом используется градиентный спуск для настройки параметров новой модели таким образом, чтобы минимизировать остаточные ошибки (то есть, разницу между предсказанными и фактическими значениями).

Ключевой параметр градиентного бустинга - это скорость обучения (learning rate), которая определяет, насколько сильно каждая новая модель влияет на итоговый прогноз. Если скорость обучения слишком большая, то алгоритм может не сойтись к оптимальному решению, а если она слишком маленькая, то обучение может занять слишком много времени.

Градиентный бустинг является одним из наиболее эффективных алгоритмов машинного обучения для решения широкого спектра задач, включая классификацию и регрессию. Он позволяет достичь высокой точности предсказаний и имеет множество возможностей для настройки гиперпараметров, что делает его очень гибким инструментом для решения различных задач. Однако, также как и другие сложные модели, градиентный бустинг может быть подвержен переобучению, поэтому важно использовать правильную стратегию выбора гиперпараметров и регуляризации.

4. Bagging - это метод машинного обучения, который использует композицию нескольких моделей для улучшения качества предсказаний. Он заключается в том, чтобы генерировать множество случайных выборок из обучающего набора данных с помощью метода бутстрэпа (bootstrap), а затем обучать отдельную модель на каждой выборке. После этого для получения итогового предсказания используется усреднение предсказаний всех моделей.

Преимуществом бэггинга является то, что он может улучшить качество предсказаний за счет уменьшения разброса (variance) модели. Кроме того, бэггинг может помочь справиться с проблемой переобучения (overfitting) путем уменьшения смещения (bias) модели.

Существует множество моделей машинного обучения, которые могут быть использованы в качестве базовых моделей для бэггинга, например, решающие деревья, логистическая регрессия, случайный лес и др.

Недостатком бэггинга является то, что он может увеличить вычислительную сложность обучения, поскольку требуется обучить несколько моделей на разных выборках. Кроме того, бэггинг может оказаться менее эффективным, если в обучающем наборе данных присутствует много шума или выбросов.

5. Stacking - это метод машинного обучения, который также использует композицию нескольких моделей для улучшения качества предсказаний. Однако, в отличие от бэггинга, он основан на идее обучения мета-модели (meta-model), которая использует предсказания других моделей в качестве входных данных.

Идея стекинга заключается в следующем. Для обучения мета-модели мы сначала обучаем несколько базовых моделей на тренировочном наборе данных. Затем, для каждого объекта из тестового набора данных мы получаем предсказания всех обученных моделей. Эти предсказания затем используются в качестве входных данных для мета-модели, которая обучается на этих предсказаниях.

Основным преимуществом стекинга является то, что он может достичь более высокой точности предсказаний, чем отдельные модели, особенно если используются разные типы моделей. Кроме того, стекинг может быть более эффективным, чем бэггинг, если используемые модели имеют различные сильные и слабые стороны.

Однако, недостатком стекинга является то, что он может быть менее устойчивым, чем отдельные модели, особенно если используется маленький набор данных. Кроме того, стекинг может быть более сложным в настройке и требует большего вычислительного времени, чем отдельные модели.

6. RandomizedSearch - это метод оптимизации гиперпараметров для моделей машинного обучения. Он представляет собой случайный поиск по заданному пространству гиперпараметров, в отличие от полного перебора (Grid Search), который ищет оптимальные значения гиперпараметров путем перебора всех возможных комбинаций.

В процессе оптимизации с помощью RandomizedSearch мы задаем диапазоны значений гиперпараметров, которые мы хотим оптимизировать, а также число случайных комбинаций, которые будут протестированы. Алгоритм случайным образом выбирает значения гиперпараметров в этих диапазонах и обучает модель на тренировочных данных с использованием этих значений. Затем оценивается качество модели на валидационных данных. Этот процесс повторяется несколько раз для разных случайных комбинаций гиперпараметров, пока не будет найдено лучшее сочетание.

Основным преимуществом RandomizedSearch является то, что он может значительно уменьшить время, затрачиваемое на оптимизацию гиперпараметров, по сравнению с полным перебором. Кроме того, он может быть более эффективным, если некоторые гиперпараметры не влияют сильно на качество модели, а некоторые - критически важны. В таких случаях случайный поиск может быстро сфокусироваться на наиболее важных гиперпараметрах.

Недостатком RandomizedSearch является то, что он не гарантирует нахождение оптимального значения гиперпараметров, особенно если заданный пространство поиска слишком ограничено. Кроме того, при использовании случайного поиска может возникнуть опасность, что некоторые комбинации гиперпараметров никогда не будут протестированы, особенно если число комбинаций невелико.

7. Random Forest - это алгоритм машинного обучения, который является расширением метода "Decision Tree". Он использует комбинацию нескольких деревьев решений (Decision Trees), чтобы получить более точные прогнозы.

Основная идея Random Forest заключается в том, чтобы создать множество деревьев решений, каждое из которых обучается на случайной подвыборке исходных данных, и случайном наборе признаков. Каждое дерево голосует за прогноз, и результирующий прогноз выбирается на основе голосования большинства.

Этот метод позволяет избежать проблемы переобучения, которая может возникнуть при обучении отдельного дерева решений на всем наборе данных. Также, случайный выбор признаков и подвыборок данных позволяет уменьшить дисперсию (variance) модели и улучшить ее обобщающую способность.

Random Forest имеет несколько преимуществ. Во-первых, он может быть использован для задач классификации и регрессии. Во-вторых, он не чувствителен к выбросам и несколько устойчив к шуму в данных. В-третьих, он может работать с большими наборами данных, содержащих множество признаков, без необходимости предварительного отбора признаков.

Недостатком метода Random Forest является то, что он может быть не очень интерпретируемым, особенно если число деревьев в лесу большое. Кроме того, он может быть не так эффективен, как некоторые другие методы машинного обучения, если данные имеют сложную структуру и зависимости между признаками.

### Метрики

1. MAE (среднеквадратичная ошибка) - это среднее значение квадратов разностей между предсказанными и фактическими значениями.
 
 MAE измеряет среднее абсолютное отклонение предсказаний модели от фактических значений целевой переменной. Для вычисления MAE необходимо для каждого примера в тестовом наборе данных вычислить абсолютную разницу между предсказанным значением и фактическим значением, а затем вычислить среднее значение этих абсолютных разностей.

Формально, для набора данных размера N, MAE может быть вычислена следующим образом:

MAE = (1/N) * Σ|y_i - ŷ_i|,

где y_i - фактическое значение целевой переменной для i-го примера, ŷ_i - предсказанное значение для i-го примера.

MAE имеет ту же размерность, что и целевая переменная и измеряется в тех же единицах, что и y. Чем меньше значение MAE, тем более точна модель в своих прогнозах.
 
2. RMSE (корень из среднеквадратичной ошибки) - это квадратный корень из MSE.
 
 RMSE измеряет среднеквадратичное отклонение предсказаний модели от фактических значений целевой переменной. Для вычисления RMSE необходимо для каждого примера в тестовом наборе данных вычислить квадрат отклонения между предсказанным значением и фактическим значением, а затем вычислить среднее значение этих квадратов. После этого извлечь корень квадратный из этого значения.

Формально, для набора данных размера N, RMSE может быть вычислена следующим образом:

RMSE = sqrt((1/N) * Σ(y_i - ŷ_i)^2),

где y_i - фактическое значение целевой переменной для i-го примера, ŷ_i - предсказанное значение для i-го примера.

RMSE также имеет ту же размерность, что и целевая переменная и измеряется в тех же единицах, что и y. Чем меньше значение RMSE, тем более точна модель в своих прогнозах.
 
3. MSE (среднеквадратичная ошибка) - это среднее значение квадратов разностей между предсказанными и фактическими значениями.
 
 MSE измеряет среднеквадратичное отклонение предсказаний модели от фактических значений целевой переменной. Для вычисления MSE необходимо для каждого примера в тестовом наборе данных вычислить квадрат отклонения между предсказанным значением и фактическим значением, а затем вычислить среднее значение этих квадратов.

Формально, для набора данных размера N, MSE может быть вычислена следующим образом:

MSE = (1/N) * Σ(y_i - ŷ_i)^2,

где y_i - фактическое значение целевой переменной для i-го примера, ŷ_i - предсказанное значение для i-го примера.

MSE имеет ту же размерность, что и целевая переменная и измеряется в тех же единицах, что и y в квадрате. Чем меньше значение MSE, тем более точна модель в своих прогнозах. Однако, MSE имеет тенденцию сильнее штрафовать за большие ошибки, чем за маленькие ошибки, поэтому для оценки качества модели иногда более предпочтительным может быть использование MAE или RMSE.
 
4. R^2 (коэффициент детерминации) - который показывает, какая доля дисперсии целевой переменной объясняется моделью.

R^2 показывает, какую долю дисперсии фактических значений целевой переменной можно объяснить моделью. Значение R^2 находится в диапазоне от 0 до 1, где 0 означает, что модель не объясняет никакой дисперсии, а 1 означает, что модель объясняет всю дисперсию.

Формально, для набора данных размера N, R^2 может быть вычислена следующим образом:

R^2 = 1 - (SS_res / SS_tot),

где SS_res - сумма квадратов остатков (расстояние между фактическими значениями целевой переменной и предсказанными значениями модели), а SS_tot - общая сумма квадратов (расстояние между фактическими значениями целевой переменной и их средним значением).

Чем ближе значение R^2 к 1, тем лучше модель объясняет данные. Однако, R^2 не всегда является достаточно информативной метрикой для оценки качества модели, так как её значение может быть сильно завышено при использовании сложных моделей с большим числом параметров. Поэтому, помимо R^2, для оценки качества модели также часто используются другие метрики, такие как MAE и RMSE.

5. MAPE (средняя абсолютная ошибка в процентах) - выраженная как отношение суммы абсолютных разностей между фактическими и предсказанными значениями к сумме фактических значений.
 
 MAPE измеряет средний процент отклонения предсказаний модели от фактических значений целевой переменной. Для вычисления MAPE необходимо для каждого примера в тестовом наборе данных вычислить абсолютное значение отклонения между предсказанным значением и фактическим значением, а затем вычислить среднее значение этих отклонений в процентах от фактических значений.

Формально, для набора данных размера N, MAPE может быть вычислена следующим образом:

MAPE = (1/N) * Σ(|y_i - ŷ_i| / y_i) * 100%,

где y_i - фактическое значение целевой переменной для i-го примера, ŷ_i - предсказанное значение для i-го примера.

MAPE выражается в процентах и показывает, насколько сильно отличаются предсказанные значения от фактических значений в среднем. Чем меньше значение MAPE, тем более точна модель в своих прогнозах. Однако, MAPE имеет некоторые недостатки, так как она может давать неопределенные значения, если фактические значения равны нулю или близки к нулю, и также не может быть использована, если некоторые фактические значения равны нулю.
 
## В итоге мы получили наилучшее качество предсказания 73.39% с помощью ансамбля "Random Forest", предварительно оптимизированного с помощью "RandomizedSearch"

